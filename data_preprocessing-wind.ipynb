{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pyspark Data-PreProcessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DateType\n",
    "schema = StructType([\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"parameter\", StructType([\n",
    "            StructField(\"WD10M\", MapType(StringType(),DoubleType()),True),\n",
    "            StructField(\"WS10M\", MapType(StringType(),DoubleType()),True),\n",
    "            StructField(\"WD2M\",  MapType(StringType(),DoubleType()),True),\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"geometry\", StructType([\n",
    "        StructField(\"coordinates\", ArrayType(DoubleType()),True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Load the json file\n",
    "data_df = spark.read.json('file:///home/talentum/test-jupyter/Daily/Adilabad_19.4000_78.3100_*.json', multiLine=True, schema=schema)\n",
    "\n",
    "#print(data_df.printSchema())\n",
    "#print(data_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.select(F.col(\"properties.parameter.*\"),F.col(\"geometry.coordinates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for features \"PS\",\"PSC\",\"T2M\",\"T2MWET\",\"T2MDEW\" \n",
    "from pyspark.sql.functions import explode\n",
    "WD2M =  data_df.select('coordinates',explode(data_df.WD2M).alias(\"Date\", \"WD2M\"))\n",
    "WD10M = data_df.select(explode(data_df.WD10M).alias(\"Date\", \"WD10M\"))\n",
    "WS10M = data_df.select(explode(data_df.WS10M).alias(\"Date\", \"WS10M\"))\n",
    "#\"WD10M\",WS10M, WD2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "final_df = WD2M.join(WD10M, WD2M.Date == WD10M.Date, 'inner') \\\n",
    ".join(WS10M , WD2M.Date == WS10M.Date, 'inner') \\\n",
    ".select(PS.Date, \"WD2M\",\"WD10M\",\"WS10M\")\n",
    "\n",
    "# another approach is by renaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-----+------+------+\n",
      "|       key|   PS|   PSC|  T2M|T2MWET|T2MDEW|\n",
      "+----------+-----+------+-----+------+------+\n",
      "|2004010105|97.34|101.68|10.65|  8.47|  6.27|\n",
      "|2004010106|97.41|101.74|10.91|  8.72|  6.52|\n",
      "|2004010107|97.48|101.76|14.73| 10.87|   7.0|\n",
      "|2004010108|97.54|101.78|18.01| 12.63|  7.26|\n",
      "|2004010109|97.55| 101.7|23.77|  14.6|  5.43|\n",
      "|2004010110|97.48|101.57|27.27| 15.38|  3.48|\n",
      "|2004010111|97.36|101.43|28.83| 15.85|  2.86|\n",
      "|2004010112|97.22|101.28|29.59| 16.07|  2.55|\n",
      "|2004010113|97.13|101.18|29.78| 16.03|  2.29|\n",
      "|2004010114|97.08|101.13| 29.4| 15.76|  2.13|\n",
      "|2004010115|97.08|101.15|28.41| 15.33|  2.26|\n",
      "|2004010116|97.11|101.23|24.76|  16.5|  8.24|\n",
      "|2004010117|97.16|101.34| 20.2| 13.82|  7.43|\n",
      "|2004010118|97.23|101.44|18.73| 12.58|  6.42|\n",
      "|2004010119|97.27| 101.5|17.75|  11.8|  5.84|\n",
      "|2004010120|97.28|101.52|16.85| 11.23|   5.6|\n",
      "|2004010121|97.26|101.52|15.95| 10.76|  5.56|\n",
      "|2004010122|97.24| 101.5|15.06| 10.37|  5.67|\n",
      "|2004010123|97.21|101.49|14.23|  10.1|  5.98|\n",
      "|2004010200| 97.2|101.49|13.48|  9.88|  6.29|\n",
      "+----------+-----+------+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df2.columns:\n",
    "#     attribute =  df2.select('coordinates',explode(df2[column]).alias(\"Date\", column))\n",
    "\n",
    "#     # Perform a left join between the empty DataFrame and the non-empty DataFrame on the \"key\" column\n",
    "#     #joined_df = empty_df.join(attribute, on=\"Date\", how=\"left\")\n",
    "\n",
    "# # Show the joined DataFrame\n",
    "# #joined_df.show()\n",
    "# final_df.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 =  df2.select('coordinates',explode(df2.PS).alias(\"key\", \"PS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(PS.show())\n",
    "# # print(PSC.show())\n",
    "\n",
    "# PS.printSchema()\n",
    "# # data_df = voter_df.filter(voter_df.VOTER_NAME.isNotNull())\n",
    "# # voter_df = voter_df.fillna(\"abc\")   if null rows are not allowed to delete than fill it with something\n",
    "# # voter_df = voter_df.dropna()       to delete null values row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define schema for the empty DataFrame\n",
    "# #schema = StructType([\n",
    "# #    StructField(\"Date\", StringType(), True)\n",
    "# #])\n",
    "\n",
    "# # Create an empty DataFrame with the specified schema\n",
    "# final_df = spark.createDataFrame([], schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
