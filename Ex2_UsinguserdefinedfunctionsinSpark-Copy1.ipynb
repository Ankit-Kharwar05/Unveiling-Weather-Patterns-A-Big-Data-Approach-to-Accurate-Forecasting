{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using user defined functions in Spark\n",
    "\n",
    "- You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "- For this exercise, we'll use our `voter_df` DataFrame, but you're going to replace the `first_name` column with the first and middle names.\n",
    "\n",
    "- The `pyspark.sql.functions` library is available under the alias `F`. The classes from `pyspark.sql.types` are already imported.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Edit the `getFirstAndMiddle()` function to return a space separated string of names, except the last entry in the names list.\n",
    "- Define the function as a user-defined function. It should return a string type.\n",
    "- Create a new column on `voter_df` called `first_and_middle_name` using your UDF.\n",
    "- Show the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Load the json file\n",
    "data_df = spark.read.json('file:///home/talentum/test-jupyter/Daily/Abohar_30.1445_74.1955_20040101_20081231.json', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- header: struct (nullable = true)\n",
      " |    |-- api: struct (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |-- end: string (nullable = true)\n",
      " |    |-- fill_value: double (nullable = true)\n",
      " |    |-- sources: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- start: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |-- messages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- parameters: struct (nullable = true)\n",
      " |    |-- PS: struct (nullable = true)\n",
      " |    |    |-- longname: string (nullable = true)\n",
      " |    |    |-- units: string (nullable = true)\n",
      " |    |-- PSC: struct (nullable = true)\n",
      " |    |    |-- longname: string (nullable = true)\n",
      " |    |    |-- units: string (nullable = true)\n",
      " |    |-- T2M: struct (nullable = true)\n",
      " |    |    |-- longname: string (nullable = true)\n",
      " |    |    |-- units: string (nullable = true)\n",
      " |    |-- T2MDEW: struct (nullable = true)\n",
      " |    |    |-- longname: string (nullable = true)\n",
      " |    |    |-- units: string (nullable = true)\n",
      " |    |-- T2MWET: struct (nullable = true)\n",
      " |    |    |-- longname: string (nullable = true)\n",
      " |    |    |-- units: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- parameter: struct (nullable = true)\n",
      " |    |    |-- PS: struct (nullable = true)\n",
      " |    |    |    |-- 2004010105: double (nullable = true)\n",
      " |    |    |    |-- 2004010106: double (nullable = true)\n",
      " |    |    |    |-- 2004010107: double (nullable = true)\n",
      " |    |    |    |-- 2004010108: double (nullable = true)\n",
      " |    |    |    |-- 2004010109: double (nullable = true)\n",
      " |    |    |    |-- 2004010110: double (nullable = true)\n",
      " |    |    |    |-- 2004010111: double (nullable = true)\n",
      " |    |    |    |-- 2004010112: double (nullable = true)\n",
      " |    |    |    |-- 2004010113: double (nullable = true)\n",
      " |    |    |    |-- 2004010114: double (nullable = true)\n",
      " |    |    |    |-- 2004010115: double (nullable = true)\n",
      " |    |    |    |-- 2004010116: double (nullable = true)\n",
      " |    |    |-- PSC: struct (nullable = true)\n",
      " |    |    |    |-- 2004010105: double (nullable = true)\n",
      " |    |    |    |-- 2004010106: double (nullable = true)\n",
      " |    |    |    |-- 2004010107: double (nullable = true)\n",
      " |    |    |    |-- 2004010108: double (nullable = true)\n",
      " |    |    |    |-- 2004010109: double (nullable = true)\n",
      " |    |    |    |-- 2004010110: double (nullable = true)\n",
      " |    |    |    |-- 2004010111: double (nullable = true)\n",
      " |    |    |    |-- 2004010112: double (nullable = true)\n",
      " |    |    |    |-- 2004010113: double (nullable = true)\n",
      " |    |    |    |-- 2004010114: double (nullable = true)\n",
      " |    |    |    |-- 2004010115: double (nullable = true)\n",
      " |    |    |    |-- 2004010116: double (nullable = true)\n",
      " |    |    |-- T2M: struct (nullable = true)\n",
      " |    |    |    |-- 2004010105: double (nullable = true)\n",
      " |    |    |    |-- 2004010106: double (nullable = true)\n",
      " |    |    |    |-- 2004010107: double (nullable = true)\n",
      " |    |    |    |-- 2004010108: double (nullable = true)\n",
      " |    |    |    |-- 2004010109: double (nullable = true)\n",
      " |    |    |    |-- 2004010110: double (nullable = true)\n",
      " |    |    |    |-- 2004010111: double (nullable = true)\n",
      " |    |    |    |-- 2004010112: double (nullable = true)\n",
      " |    |    |    |-- 2004010113: double (nullable = true)\n",
      " |    |    |    |-- 2004010114: double (nullable = true)\n",
      " |    |    |    |-- 2004010115: double (nullable = true)\n",
      " |    |    |    |-- 2004010116: double (nullable = true)\n",
      " |    |    |-- T2MDEW: struct (nullable = true)\n",
      " |    |    |    |-- 2004010105: double (nullable = true)\n",
      " |    |    |    |-- 2004010106: double (nullable = true)\n",
      " |    |    |    |-- 2004010107: double (nullable = true)\n",
      " |    |    |    |-- 2004010108: double (nullable = true)\n",
      " |    |    |    |-- 2004010109: double (nullable = true)\n",
      " |    |    |    |-- 2004010110: double (nullable = true)\n",
      " |    |    |    |-- 2004010111: double (nullable = true)\n",
      " |    |    |    |-- 2004010112: double (nullable = true)\n",
      " |    |    |    |-- 2004010113: double (nullable = true)\n",
      " |    |    |    |-- 2004010114: double (nullable = true)\n",
      " |    |    |    |-- 2004010115: double (nullable = true)\n",
      " |    |    |    |-- 2004010116: double (nullable = true)\n",
      " |    |    |-- T2MWET: struct (nullable = true)\n",
      " |    |    |    |-- 2004010105: double (nullable = true)\n",
      " |    |    |    |-- 2004010106: double (nullable = true)\n",
      " |    |    |    |-- 2004010107: double (nullable = true)\n",
      " |    |    |    |-- 2004010108: double (nullable = true)\n",
      " |    |    |    |-- 2004010109: double (nullable = true)\n",
      " |    |    |    |-- 2004010110: double (nullable = true)\n",
      " |    |    |    |-- 2004010111: double (nullable = true)\n",
      " |    |    |    |-- 2004010112: double (nullable = true)\n",
      " |    |    |    |-- 2004010113: double (nullable = true)\n",
      " |    |    |    |-- 2004010114: double (nullable = true)\n",
      " |    |    |    |-- 2004010115: double (nullable = true)\n",
      " |    |    |    |-- 2004010116: double (nullable = true)\n",
      " |-- times: struct (nullable = true)\n",
      " |    |-- data: double (nullable = true)\n",
      " |    |-- process: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data_1_df = data_df.withColumn(\"Parameter\", data_df.properties.parameter) \\\n",
    ".drop('properties','parameters','times','type','geometry','header','messages') \\\n",
    ".withColumn(\"PS\", col('Parameter')['PS']) \\\n",
    ".withColumn(\"PSC\", col('Parameter')['PSC']) \\\n",
    ".drop(\"Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PS: struct (nullable = true)\n",
      " |    |-- 2004010105: double (nullable = true)\n",
      " |    |-- 2004010106: double (nullable = true)\n",
      " |    |-- 2004010107: double (nullable = true)\n",
      " |    |-- 2004010108: double (nullable = true)\n",
      " |    |-- 2004010109: double (nullable = true)\n",
      " |    |-- 2004010110: double (nullable = true)\n",
      " |    |-- 2004010111: double (nullable = true)\n",
      " |    |-- 2004010112: double (nullable = true)\n",
      " |    |-- 2004010113: double (nullable = true)\n",
      " |    |-- 2004010114: double (nullable = true)\n",
      " |    |-- 2004010115: double (nullable = true)\n",
      " |    |-- 2004010116: double (nullable = true)\n",
      " |-- PSC: struct (nullable = true)\n",
      " |    |-- 2004010105: double (nullable = true)\n",
      " |    |-- 2004010106: double (nullable = true)\n",
      " |    |-- 2004010107: double (nullable = true)\n",
      " |    |-- 2004010108: double (nullable = true)\n",
      " |    |-- 2004010109: double (nullable = true)\n",
      " |    |-- 2004010110: double (nullable = true)\n",
      " |    |-- 2004010111: double (nullable = true)\n",
      " |    |-- 2004010112: double (nullable = true)\n",
      " |    |-- 2004010113: double (nullable = true)\n",
      " |    |-- 2004010114: double (nullable = true)\n",
      " |    |-- 2004010115: double (nullable = true)\n",
      " |    |-- 2004010116: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# data_1_df.show()\n",
    "type(data_1_df)\n",
    "print(data_1_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS\n",
      "PSC\n"
     ]
    }
   ],
   "source": [
    "for column in data_1_df.columns:\n",
    "    \n",
    "    data_2_df = data_1_df.withColumn(\"some\", data_1_df.column[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_2_df = data_1_df.withColumn(\"2004010105\", data_1_df.PS['2004010105'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PS: struct (nullable = true)\n",
      " |    |-- 2004010105: double (nullable = true)\n",
      " |    |-- 2004010106: double (nullable = true)\n",
      " |    |-- 2004010107: double (nullable = true)\n",
      " |    |-- 2004010108: double (nullable = true)\n",
      " |    |-- 2004010109: double (nullable = true)\n",
      " |    |-- 2004010110: double (nullable = true)\n",
      " |    |-- 2004010111: double (nullable = true)\n",
      " |    |-- 2004010112: double (nullable = true)\n",
      " |    |-- 2004010113: double (nullable = true)\n",
      " |    |-- 2004010114: double (nullable = true)\n",
      " |    |-- 2004010115: double (nullable = true)\n",
      " |    |-- 2004010116: double (nullable = true)\n",
      " |-- PSC: struct (nullable = true)\n",
      " |    |-- 2004010105: double (nullable = true)\n",
      " |    |-- 2004010106: double (nullable = true)\n",
      " |    |-- 2004010107: double (nullable = true)\n",
      " |    |-- 2004010108: double (nullable = true)\n",
      " |    |-- 2004010109: double (nullable = true)\n",
      " |    |-- 2004010110: double (nullable = true)\n",
      " |    |-- 2004010111: double (nullable = true)\n",
      " |    |-- 2004010112: double (nullable = true)\n",
      " |    |-- 2004010113: double (nullable = true)\n",
      " |    |-- 2004010114: double (nullable = true)\n",
      " |    |-- 2004010115: double (nullable = true)\n",
      " |    |-- 2004010116: double (nullable = true)\n",
      " |-- 2004010105: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# data_1_df.show()\n",
    "type(data_2_df)\n",
    "print(data_2_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                  PS|                 PSC|2004010105|\n",
      "+--------------------+--------------------+----------+\n",
      "|[99.45, 99.49, 99...|[101.64, 101.68, ...|     99.45|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_col = 'Cal'\n",
    "cols_minus_header = df.columns\n",
    "cols_minus_header.remove(header_col)\n",
    "\n",
    "df1 = (df\n",
    "       .groupBy()\n",
    "       .pivot('Cal')\n",
    "       .agg(F.first(F.array(cols_minus_header)))\n",
    "       .withColumn(header_col, F.array(*map(F.lit, cols_minus_header)))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+--------------------+--------------------+-------------+-------+----------+\n",
      "|            geometry|              header|messages|          parameters|          properties|        times|   type|     start|\n",
      "+--------------------+--------------------+--------+--------------------+--------------------+-------------+-------+----------+\n",
      "|[[74.1955, 30.144...|[[POWER Hourly AP...|      []|[[Surface Pressur...|[[[99.45, 99.49, ...|[2.035, 2.25]|Feature|2004010105|\n",
      "+--------------------+--------------------+--------+--------------------+--------------------+-------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "start_date_str = data_df.withColumn(\"start\", col('header.start'))\n",
    "end_date_str = \"2000123123\"\n",
    "\n",
    "type(start_date_str)\n",
    "\n",
    "start_date_str.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Function to convert string in format \"YYYYMMDDHH\" to datetime object\n",
    "def parse_date(input_date):\n",
    "    return datetime.datetime.strptime(input_date, \"%Y%m%d%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strptime() argument 1 must be str, not Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6fbb4eeb9c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_date_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-92bc9dd0e5dc>\u001b[0m in \u001b[0;36mparse_date\u001b[0;34m(input_date)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Function to convert string in format \"YYYYMMDDHH\" to datetime object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y%m%d%H\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be str, not Column"
     ]
    }
   ],
   "source": [
    "start_date = parse_date(start_date_str)\n",
    "end_date = parse_date(end_date_str)\n",
    "\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through dates and print in the desired format\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    print(current_date.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    current_date += datetime.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- parameter: struct (nullable = true)\n",
      " |    |    |-- PS: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: double (valueContainsNull = true)\n",
      " |    |    |-- PSC: string (nullable = true)\n",
      " |    |    |-- T2M_MAX: string (nullable = true)\n",
      " |    |    |-- T2M_MIN: string (nullable = true)\n",
      " |    |    |-- T2MWET: string (nullable = true)\n",
      " |    |    |-- T2MDEW: string (nullable = true)\n",
      " |    |    |-- T2M: string (nullable = true)\n",
      " |    |    |-- T2M_RANGE: string (nullable = true)\n",
      " |    |    |-- TS: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: double (valueContainsNull = true)\n",
      " |    |    |-- WS10M: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|          properties|\n",
      "+--------------------+\n",
      "|[[[2004010105 -> ...|\n",
      "+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"parameter\", StructType([\n",
    "            StructField(\"PS\", MapType(StringType(),DoubleType()),True),\n",
    "            StructField(\"PSC\", StringType(),True),\n",
    "            StructField(\"T2M_MAX\", StringType(),True),\n",
    "            StructField(\"T2M_MIN\", StringType(),True),\n",
    "            StructField(\"T2MWET\", StringType(),True),\n",
    "            StructField(\"T2MDEW\", StringType(),True),\n",
    "            StructField(\"T2M\", StringType(),True),\n",
    "            StructField(\"T2M_RANGE\", StringType(),True),\n",
    "            StructField(\"TS\", MapType(StringType(),DoubleType()),True),\n",
    "            StructField(\"WS10M\", StringType(),True),\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load the json file\n",
    "data_df = spark.read.json('file:///home/talentum/test-jupyter/Daily/Abohar_30.1445_74.1955_20040101_20081231.json', multiLine=True, schema=schema)\n",
    "\n",
    "print(data_df.printSchema())\n",
    "print(data_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.withColumn\n",
    "final_df = data_df.withColumn(\"PS\", data_df.properties.parameter.PS) \\\n",
    ".withColumn(\"PSC\", data_df.properties.parameter.PSC) \\\n",
    ".withColumn(\"TS\", data_df.properties.parameter.TS) \\\n",
    ".withColumn(\"T2M\", data_df.properties.parameter.T2M) \\\n",
    ".withColumn(\"T2M_MIN\", data_df.properties.parameter.T2M_MIN) \\\n",
    ".withColumn(\"T2M_MAX\", data_df.properties.parameter.T2M_MAX).show()\n",
    "\n",
    "#data_df['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = data_df.select(\"properties.parameter.TS\")\n",
    "df2 =  data_df.select(col(\"properties.parameter.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       key|value|\n",
      "+----------+-----+\n",
      "|2004010105|99.45|\n",
      "|2004010106|99.49|\n",
      "|2004010107|99.53|\n",
      "|2004010108| 99.6|\n",
      "|2004010109|99.65|\n",
      "|2004010110|99.62|\n",
      "|2004010111|99.52|\n",
      "|2004010112|99.44|\n",
      "|2004010113|99.37|\n",
      "|2004010114|99.31|\n",
      "|2004010115|99.28|\n",
      "|2004010116|99.28|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df3 = df2.explode()\n",
    "from pyspark.sql.functions import explode\n",
    "df2.select(explode(df2.PS).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_df = voter_df.filter(voter_df.VOTER_NAME.isNotNull())\n",
    "# voter_df = voter_df.fillna(\"abc\")   if null rows are not allowed to delete than fill it with something\n",
    "# voter_df = voter_df.dropna()       to delete null values row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
