{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory containing your JSON files\n",
    "json_files_directory = \"D://Project//CDAC-Project//hourly//wind\"\n",
    "\n",
    "with open('D://Project//CDAC-Project//Indian_Cities_Database.csv', 'r') as csvfile:\n",
    "    coordinates = csv.DictReader(csvfile)\n",
    "    for row in coordinates:\n",
    "        city = row['City']\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "\n",
    "    # Get a list of all JSON files in the directory\n",
    "    json_files = [file for file in os.listdir(json_files_directory) if file.startswith(f'{city}_*')]\n",
    "\n",
    "    # Create an empty DataFrame to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each JSON file and append its data to the combined DataFrame\n",
    "    for json_file in json_files:\n",
    "        json_file_path = os.path.join(json_files_directory, json_file)\n",
    "\n",
    "        # Read JSON data from file\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract parameter data\n",
    "        parameter_data = json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(parameter_data)\n",
    "\n",
    "        # Transpose the DataFrame for a better structure\n",
    "        # df = df.transpose()\n",
    "\n",
    "        # Convert index (date) to datetime objects with a more flexible approach\n",
    "        #df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "        df.index = pd.to_datetime(df.index, format='%Y%m%d%H')\n",
    "\n",
    "        # Append the DataFrame to the combined DataFrame\n",
    "        combined_df = combined_df.append(df)\n",
    "\n",
    "    # Display the combined DataFrame\n",
    "    #print(combined_df)\n",
    "\n",
    "\n",
    "    # Specify the CSV file path where you want to save the combined DataFrame\n",
    "    csv_file_path = '/Users/darshmac/Documents/Cdac Project/combined_data.csv'\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    # Display a message indicating that the CSV file has been saved\n",
    "    print(f\"The combined data has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a2221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Specify the directory containing your JSON files\n",
    "json_files_directory = \"D://Project//hourly//Sample\"\n",
    "\n",
    "# Specify the path to the CSV file containing common names and corresponding city names\n",
    "city_names_csv_path = 'D://Project//CDAC-Project//Indian_Cities_Database.csv'\n",
    "\n",
    "# Read common names and city names from the CSV file\n",
    "city_names_df = pd.read_csv(city_names_csv_path)\n",
    "\n",
    "# Create a dictionary to store DataFrames for each city\n",
    "city_dataframes = {}\n",
    "\n",
    "# Loop through each row in the common names DataFrame\n",
    "for index, row in city_names_df.iterrows():\n",
    "    # Construct the expected JSON file name based on the common name\n",
    "    json_file_pattern = os.path.join(json_files_directory, row['City'] + '*.json')\n",
    "\n",
    "    # Use glob to get a list of matching files\n",
    "    matching_files = glob(json_file_pattern)\n",
    "\n",
    "    # Loop through each matching file and read JSON data\n",
    "    for json_file_path in matching_files:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract parameter data\n",
    "        parameter_data = json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(parameter_data)\n",
    "\n",
    "        # Append the DataFrame to the city's DataFrame\n",
    "        if row['City'] not in city_dataframes:\n",
    "            city_dataframes[row['City']] = pd.DataFrame()\n",
    "        city_dataframes[row['City']] = pd.concat([city_dataframes[row['City']],df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c1606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined data for Abohar has been saved to D://Project//hourly//Sample\\Abohar_combined_data.csv\n",
      "The combined data for Adilabad has been saved to D://Project//hourly//Sample\\Adilabad_combined_data.csv\n",
      "The combined data for Agartala has been saved to D://Project//hourly//Sample\\Agartala_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "# Save each city's DataFrame to a separate CSV file\n",
    "for city, city_df in city_dataframes.items():\n",
    "    csv_file_path = os.path.join(json_files_directory,f\"{city}_combined_data.csv\")\n",
    "    city_df.to_csv(csv_file_path, index=True, index_label=\"Date\")\n",
    "    print(f\"The combined data for {city} has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0387d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `df.append` not found.\n"
     ]
    }
   ],
   "source": [
    "df.ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b078d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e025d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined data for Abohar has been saved to D://Project//hourly//Sample//wind_d\\Abohar_combined_data.csv\n",
      "The combined data for Adilabad has been saved to D://Project//hourly//Sample//wind_d\\Adilabad_combined_data.csv\n",
      "The combined data for Agartala has been saved to D://Project//hourly//Sample//wind_d\\Agartala_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Specify the directory containing your tem files\n",
    "temprature_files_directory = \"D://Project//hourly//Sample\"\n",
    "\n",
    "# Specify the directory containing your wind files\n",
    "wind_files_directory = \"D://Project//hourly//Sample//wind_d\"\n",
    "\n",
    "# Specify the path to the CSV file containing common names and corresponding city names\n",
    "city_names_csv_path = 'D://Project//CDAC-Project//Indian_Cities_Database.csv'\n",
    "\n",
    "# Read common names and city names from the CSV file\n",
    "city_names_df = pd.read_csv(city_names_csv_path)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each row in the common names DataFrame\n",
    "for index, row in city_names_df.iterrows():\n",
    "    # Construct the expected JSON file name based on the common name\n",
    "    temprature_file_pattern = os.path.join(temprature_files_directory, row['City'] + '*.json')\n",
    "    \n",
    "    # Create a dictionary to store DataFrames for each city\n",
    "    city_dataframes = {}\n",
    "    df1 = {}\n",
    "    temp_flag = False\n",
    "    \n",
    "    # Use glob to get a list of matching files\n",
    "    matching_files = glob(temprature_file_pattern)\n",
    "\n",
    "    # Loop through each matching file and read JSON data\n",
    "    for temprature_file_path in matching_files:\n",
    "        with open(temprature_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract parameter data\n",
    "        parameter_data = json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(parameter_data)\n",
    "\n",
    "        # Append the DataFrame to the city's DataFrame\n",
    "        if row['City'] not in city_dataframes:\n",
    "            city_dataframes[row['City']] = pd.DataFrame()\n",
    "        city_dataframes[row['City']] = pd.concat([city_dataframes[row['City']],df], ignore_index=False)\n",
    "        #print(city_dataframes[row['City']].head())\n",
    "        temp_flag = True\n",
    "    \n",
    "    # Construct the expected JSON file name based on the common name\n",
    "    wind_file_pattern = os.path.join(wind_files_directory, row['City'] + '*.json')\n",
    "    \n",
    "    # Create a dictionary to store DataFrames for each city\n",
    "    city_wind_dataframes = {}\n",
    "    wind_flag = False\n",
    "    \n",
    "    # Use glob to get a list of matching files\n",
    "    matching_files = glob(wind_file_pattern)\n",
    "    \n",
    "    \n",
    "    # Loop through each matching file and read JSON data\n",
    "    for wind_file_path in matching_files:\n",
    "        with open(wind_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract parameter data\n",
    "        parameter_data = json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(parameter_data)\n",
    "\n",
    "        # Append the DataFrame to the city's DataFrame\n",
    "        if row['City'] not in city_wind_dataframes:\n",
    "            city_wind_dataframes[row['City']] = pd.DataFrame()\n",
    "        city_wind_dataframes[row['City']] = pd.concat([city_wind_dataframes[row['City']],df], ignore_index=False)\n",
    "        #print(city_wind_dataframes[row['City']].head())\n",
    "        wind_flag = True\n",
    "    \n",
    "    if wind_flag & temp_flag:\n",
    "        df1[row['City']] = pd.merge(city_dataframes[row['City']],city_wind_dataframes[row['City']], left_on=city_dataframes[row['City']].index, right_index=True, how='inner')\n",
    "        df1[row['City']] = df1[row['City']].drop(columns=['key_0','PSC_y'])\n",
    "    elif wind_flag :\n",
    "        df1[row['City']] = city_wind_dataframes[row['City']]\n",
    "    elif temp_flag :\n",
    "        df1[row['City']] = city_dataframes[row['City']]\n",
    "\n",
    "    \n",
    "    # Save each city's DataFrame to a separate CSV file\n",
    "    for city, city_df in df1.items():\n",
    "            csv_file_path = os.path.join(wind_files_directory,f\"{city}_combined_data.csv\")\n",
    "            city_df.to_csv(csv_file_path, index=True, index_label=\"Date\")\n",
    "            print(f\"The combined data for {city} has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52765851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined data for key_0 has been saved to D://Project//hourly//Sample//wind\\key_0_combined_data.csv\n",
      "The combined data for T2M has been saved to D://Project//hourly//Sample//wind\\T2M_combined_data.csv\n",
      "The combined data for T2MDEW has been saved to D://Project//hourly//Sample//wind\\T2MDEW_combined_data.csv\n",
      "The combined data for T2MWET has been saved to D://Project//hourly//Sample//wind\\T2MWET_combined_data.csv\n",
      "The combined data for PS has been saved to D://Project//hourly//Sample//wind\\PS_combined_data.csv\n",
      "The combined data for PSC_x has been saved to D://Project//hourly//Sample//wind\\PSC_x_combined_data.csv\n",
      "The combined data for WS2M has been saved to D://Project//hourly//Sample//wind\\WS2M_combined_data.csv\n",
      "The combined data for QV2M has been saved to D://Project//hourly//Sample//wind\\QV2M_combined_data.csv\n",
      "The combined data for RH2M has been saved to D://Project//hourly//Sample//wind\\RH2M_combined_data.csv\n",
      "The combined data for PRECTOTCORR has been saved to D://Project//hourly//Sample//wind\\PRECTOTCORR_combined_data.csv\n",
      "The combined data for PSC_y has been saved to D://Project//hourly//Sample//wind\\PSC_y_combined_data.csv\n",
      "The combined data for key_0 has been saved to D://Project//hourly//Sample//wind\\key_0_combined_data.csv\n",
      "The combined data for T2M has been saved to D://Project//hourly//Sample//wind\\T2M_combined_data.csv\n",
      "The combined data for T2MDEW has been saved to D://Project//hourly//Sample//wind\\T2MDEW_combined_data.csv\n",
      "The combined data for T2MWET has been saved to D://Project//hourly//Sample//wind\\T2MWET_combined_data.csv\n",
      "The combined data for PS has been saved to D://Project//hourly//Sample//wind\\PS_combined_data.csv\n",
      "The combined data for PSC_x has been saved to D://Project//hourly//Sample//wind\\PSC_x_combined_data.csv\n",
      "The combined data for WS2M has been saved to D://Project//hourly//Sample//wind\\WS2M_combined_data.csv\n",
      "The combined data for QV2M has been saved to D://Project//hourly//Sample//wind\\QV2M_combined_data.csv\n",
      "The combined data for RH2M has been saved to D://Project//hourly//Sample//wind\\RH2M_combined_data.csv\n",
      "The combined data for PRECTOTCORR has been saved to D://Project//hourly//Sample//wind\\PRECTOTCORR_combined_data.csv\n",
      "The combined data for PSC_y has been saved to D://Project//hourly//Sample//wind\\PSC_y_combined_data.csv\n",
      "The combined data for key_0 has been saved to D://Project//hourly//Sample//wind\\key_0_combined_data.csv\n",
      "The combined data for T2M has been saved to D://Project//hourly//Sample//wind\\T2M_combined_data.csv\n",
      "The combined data for T2MDEW has been saved to D://Project//hourly//Sample//wind\\T2MDEW_combined_data.csv\n",
      "The combined data for T2MWET has been saved to D://Project//hourly//Sample//wind\\T2MWET_combined_data.csv\n",
      "The combined data for PS has been saved to D://Project//hourly//Sample//wind\\PS_combined_data.csv\n",
      "The combined data for PSC_x has been saved to D://Project//hourly//Sample//wind\\PSC_x_combined_data.csv\n",
      "The combined data for WS2M has been saved to D://Project//hourly//Sample//wind\\WS2M_combined_data.csv\n",
      "The combined data for QV2M has been saved to D://Project//hourly//Sample//wind\\QV2M_combined_data.csv\n",
      "The combined data for RH2M has been saved to D://Project//hourly//Sample//wind\\RH2M_combined_data.csv\n",
      "The combined data for PRECTOTCORR has been saved to D://Project//hourly//Sample//wind\\PRECTOTCORR_combined_data.csv\n",
      "The combined data for PSC_y has been saved to D://Project//hourly//Sample//wind\\PSC_y_combined_data.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Agra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m         city_wind_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     73\u001b[0m     city_wind_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([city_wind_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]],df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(city_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]],city_wind_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39mcity_dataframes[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mindex, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Save each city's DataFrame to a separate CSV file\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city, city_df \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Agra'"
     ]
    }
   ],
   "source": [
    "    \n",
    "    # Save each city's DataFrame to a separate CSV file\n",
    "    for city, city_df in df.items():\n",
    "      csv_file_path = os.path.join(wind_files_directory,f\"{city}_combined_data.csv\")\n",
    "        city_df.to_csv(csv_file_path, index=True, index_label=\"Date\")\n",
    "        print(f\"The combined data for {city} has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Specify the directory containing your main JSON files\n",
    "main_json_files_directory = '/Users/darshmac/Documents/Cdac Project/Sample'\n",
    "\n",
    "# Specify the directory containing JSON files with extra information\n",
    "extra_json_files_directory = '/Users/darshmac/Documents/Cdac Project/sample_col'\n",
    "\n",
    "# Specify the path to the CSV file containing common names and corresponding city names\n",
    "common_names_csv_path = '/Users/darshmac/Documents/Cdac Project/CDAC-Project/CDAC_Project/CDAC-Project/Indian_Cities_Database.csv'\n",
    "\n",
    "# Read common names and city names from the CSV file\n",
    "common_names_df = pd.read_csv(common_names_csv_path)\n",
    "\n",
    "# Create a dictionary to store DataFrames for each city\n",
    "city_dataframes = {}\n",
    "\n",
    "# Loop through each row in the common names DataFrame\n",
    "for index, row in common_names_df.iterrows():\n",
    "    # Construct the expected JSON file name based on the common name\n",
    "    main_json_file_pattern = os.path.join(main_json_files_directory, row['City'] + '*.json')\n",
    "    extra_json_file_pattern = os.path.join(extra_json_files_directory, row['City'] + '*.json')\n",
    "\n",
    "    # Use glob to get a list of matching files for main and extra information\n",
    "    main_matching_files = glob(main_json_file_pattern)\n",
    "    extra_matching_files = glob(extra_json_file_pattern)\n",
    "\n",
    "    # Reset the index before the loop\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each matching file and read main JSON data\n",
    "    for main_json_file_path in main_matching_files:\n",
    "        with open(main_json_file_path, 'r') as main_file:\n",
    "            main_json_data = json.load(main_file)\n",
    "\n",
    "        # Extract parameter data\n",
    "        parameter_data = main_json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(parameter_data)\n",
    "\n",
    "    # Loop through each matching file for extra information and read extra JSON data\n",
    "    for extra_json_file_path in extra_matching_files:\n",
    "        with open(extra_json_file_path, 'r') as extra_file:\n",
    "            extra_json_data = json.load(extra_file)\n",
    "\n",
    "        # Extract extra information\n",
    "        extra_info_data = extra_json_data['properties']['parameter']\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        extra_df = pd.DataFrame(extra_info_data)\n",
    "\n",
    "        # Convert the index of extra_df to the same data type as df's index\n",
    "        extra_df.index = extra_df.index.astype(df.index.dtype)\n",
    "\n",
    "        # Merge the main DataFrame with the extra DataFrame on the original index\n",
    "        df = pd.merge(df, extra_df, on=None, how='inner')\n",
    "\n",
    "    # Append the DataFrame to the city's DataFrame\n",
    "    if row['City'] not in city_dataframes:\n",
    "        city_dataframes[row['City']] = pd.DataFrame()\n",
    "    city_dataframes[row['City']] = pd.concat([city_dataframes[row['City']], df], axis=0)\n",
    "\n",
    "# Save each city's DataFrame to a separate CSV file\n",
    "for city, city_df in city_dataframes.items():\n",
    "    csv_file_path = f'/Users/darshmac/Documents/Cdac Project/{city}_combined_data_with_extra.csv'\n",
    "    city_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"The combined data for {city} with extra information has been saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
